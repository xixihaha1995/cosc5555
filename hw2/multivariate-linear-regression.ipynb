{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "\n",
    "This notebook will guide you through implementation of **multivariate linear regression** to to solve the **polynomial regression** problem:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 +  \\theta_3 x^3 + \\theta_4 x^4\n",
    "= \\boldsymbol{\\theta}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\theta_3 \\\\ \\theta_4\\end{bmatrix}, \n",
    "\\qquad\n",
    "\\mathbf{x} = \\begin{bmatrix}1 \\\\ x \\\\ x^2 \\\\ x^3 \\\\ x^4\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Below, you will follow steps to \n",
    "\n",
    "1. Implement the cost function for multivarate linear regression\n",
    "1. Compare vectorized code with for-loops\n",
    "1. Implement the normal equations method to solve a multivariate linear regression problem\n",
    "1. Implement gradient descent for multivariate linear regression\n",
    "1. Experiment with feature normalization to improve the convergence of gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Run this code to set up the helper functions. The function ``feature_expansion`` accepts an vector of $n$ scalar x values and returns an $n \\times 5$ data matrix by applying the feature expansion $x \\mapsto [1, x, x^2, x^3, x^4]$ to each scalar $x$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_expansion(x, deg):\n",
    "    if x.ndim > 1:\n",
    "        raise ValueError('x should be a 1-dimensional array')\n",
    "    m = x.shape\n",
    "    x_powers = [x**k for k in range(0,deg+1)]\n",
    "    X = np.stack( x_powers, axis=1 )\n",
    "\n",
    "    return X\n",
    "\n",
    "def plot_model(X_test, theta):\n",
    "    '''\n",
    "    Note: uses globals x, y, x_test, which are assigned below\n",
    "    when the dataset is created. Don't overwrite these variables.\n",
    "    '''\n",
    "    y_test = np.dot(X_test, theta)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x_test, y_test)\n",
    "    plt.legend(['Test', 'Train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) List comprehensions\n",
    "\n",
    "Read about Python list comprehensions. Explain what is happening in the following line of code\n",
    "\n",
    "```python\n",
    "x_powers = [x**k for k in range(0,deg+1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Your answer here* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data set for polynomial regression\n",
    "\n",
    "Read and run the code below. This generates data from a fourth-degree polynomial and then uses feature expansion to set up the problem of learning the polynomial as multivariate linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create random set of m training x values between -5 and 5\n",
    "m = 100\n",
    "x = np.random.rand(m)*10 - 5   \n",
    "\n",
    "# Create evenly spaced test x values (for plotting)\n",
    "x_test  = np.linspace(-5, 5, 100)\n",
    "m_test  = len(x_test);\n",
    "\n",
    "# Feature expansion for training and test x values\n",
    "deg = 4\n",
    "X      = feature_expansion(x, deg)\n",
    "X_test = feature_expansion(x_test, deg)\n",
    "\n",
    "n = deg + 1   # total number of features including the '1' feature\n",
    "\n",
    "# Define parameters (theta) and generate y values\n",
    "theta = 0.1*np.array([1, 1, 10, 0.5, -0.5]);\n",
    "y = np.dot(X, theta) + np.random.randn(m)   # polynomial plus noise\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(x, y)\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the feature expansion for a single training example\n",
    "print(x[0]) #original data \n",
    "print(X[0]) #data with feature expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the cost function\n",
    "Follow the instructions to implement the following cost function for multivariate linear regression:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^n(h_{\\theta}(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 points)  Cost function with loops \n",
    "First, implement the cost function using a for-loops: `cost_function_loops`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_loops(X, y, theta):  \n",
    "    '''\n",
    "    Compute the cost function for a particular data set and \n",
    "    hypothesis (parameter vector)\n",
    "    \n",
    "    \n",
    "    Inputs: \n",
    "        X       m x n data matrix\n",
    "        y       training output (length m vector)\n",
    "        theta   parameters (length n vector)\n",
    "    Output:\n",
    "        cost    the value of the cost function (scalar)\n",
    "    '''\n",
    "    # TODO: write correct code to implement the cost function given above\n",
    "    # Use for-loops for this function\n",
    "    cost = 0\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 points)  Vectorized cost function \n",
    "Now, implment the same cost function but now WITHOUT any for-loops. You should be using NumPy. This is a \"vectorized\" version of the cost function: `cost_function_vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_vec(X, y, theta):  \n",
    "    '''\n",
    "    No for-loops allowed! \n",
    "    \n",
    "    Compute the cost function for a particular data set and \n",
    "    hypothesis (parameter vector)\n",
    "    \n",
    "    Inputs: \n",
    "        X       m x n data matrix\n",
    "        y       training output (length m vector)\n",
    "        theta   parameters (length n vector)\n",
    "    Output:\n",
    "        cost    the value of the cost function (scalar)\n",
    "    '''\n",
    "    \n",
    "    # TODO: write correct code to implement the cost function given above\n",
    "    # You CANNOT use for-loops here!\n",
    "    cost = 0\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the cost function\n",
    "Run this to test your cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "theta_random = np.random.rand(n)\n",
    "theta_zeros  = np.zeros(n)\n",
    "theta_ones   = np.ones(n)\n",
    "\n",
    "print(\"cost_function_loops\")\n",
    "print(\"==\"*10)\n",
    "print( \"Cost (random): %.2f\" % cost_function_loops(X, y, theta_random))  # prints 54523.64\n",
    "print( \"Cost (zeros): %.2f\" % cost_function_loops(X, y, theta_zeros))   # prints 845.65\n",
    "print( \"Cost (ones): %.2f\" % cost_function_loops(X, y, theta_ones))    # prints 2524681.08\n",
    "print()\n",
    "print(\"cost_function_vec\")\n",
    "print(\"==\"*10)\n",
    "print( \"Cost (random): %.2f\" % cost_function_vec(X, y, theta_random))  # prints 54523.64\n",
    "print( \"Cost (zeros): %.2f\" % cost_function_vec(X, y, theta_zeros))   # prints 845.65\n",
    "print( \"Cost (ones): %.2f\" % cost_function_vec(X, y, theta_ones))    # prints 2524681.08\n",
    "print()\n",
    "#Note: The for-loop and vectorized cost function implementations should return the EXACT \n",
    "# same results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points)  Time cost functions \n",
    "\n",
    "- Run `cost_function_loops` and `cost_function_vec` each 100 times for `theta_random` and the `X` and `y` given above\n",
    "- Print out the mean and standard deviation across all runs for each function \n",
    "- Report which function is faster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "theta_random = np.random.rand(n)\n",
    "##################\n",
    "# TODO: implement your code here #\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Write answer here: report which function is faster* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the rest of this notebook, we will use the vectorized implementation of the cost function \n",
    "Run the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = cost_function_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure it works \n",
    "cost_function(X, y, theta_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7 points) Implement first training algorithm: normal equations\n",
    "Implement a *vectorized* version of the normal equations! If you use for-loops you will not get full credit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equations(X, y):\n",
    "    '''\n",
    "    Train a linear regression model using the normal equations\n",
    "\n",
    "    Inputs: \n",
    "        X       m x n data matrix\n",
    "        y       training output (length m vector)\n",
    "    Output:\n",
    "        theta   parameters (length n vector)\n",
    "\n",
    "    '''\n",
    "    # TODO: write correct code to find theta using the normal equations\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use normal equations to fit the model\n",
    "Run this code to test your implementation of the normal equations. If it runs properly you will see a curve that fits the data well. Note the value of the cost function for ``theta_normal_equations``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_normal_equations = normal_equations(X, y)\n",
    "plot_model(X_test, theta_normal_equations)\n",
    "print (\"Cost function: %.2f\" % cost_function(X, y, theta_normal_equations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7 points) Implement second training algorithm: (vectorized) gradient descent\n",
    "\n",
    "Implement gradient descent for multivariate linear regression. Make sure your solution is vectorized. If you use for-loops to compute the gradient you will not receive full credit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent( X, y, alpha, iters, theta=None ):\n",
    "    '''\n",
    "    Train a linear regression model by gradient descent\n",
    "\n",
    "    Inputs: \n",
    "        X       m x n data matrix\n",
    "        y       training output (length m vector)\n",
    "        alpha   step size\n",
    "        iters   number of iterations\n",
    "        theta   initial parameter values (length n vector; optional)\n",
    "    \n",
    "    Output:\n",
    "        theta      learned parameters (length n vector)\n",
    "        J_history  trace of cost function value in each iteration\n",
    "\n",
    "    '''\n",
    "\n",
    "    m,n = X.shape\n",
    "    \n",
    "    if theta is None:\n",
    "        theta = np.zeros(n)\n",
    "    \n",
    "    # For recording cost function value during gradient descent\n",
    "    J_history = np.zeros(iters)\n",
    "\n",
    "    for i in range(0, iters):\n",
    "        \n",
    "        # TODO: compute gradient (vectorized) and update theta\n",
    "        \n",
    "        # Record cost function\n",
    "        J_history[i] = cost_function(X, y, theta)\n",
    "        \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6 points) Use gradient descent to train the model\n",
    "* Write code to call your ``gradient_descent`` method to learn parameter\n",
    "* Plot the model fit (use ``plot_model``)\n",
    "* Plot the cost function vs. iteration to help assess convergence\n",
    "* Print the final value of the cost function\n",
    "* Experiment with different step sizes and numbers of iterations until you can find a good hypothesis. Try to match the cost function value from ``normal_equations`` to two decimal places. How many iterations does this take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Write your answers here* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (10 points) Gradient descent with feature normalization\n",
    "You should have observed\n",
    "that it takes many iterations of gradient descent to match the cost\n",
    "function value achieved by the normal equations. Now\n",
    "you will implement feature normalization to improve the convergence\n",
    "of gradient descent. Remember that the formula\n",
    "for feature normalization is:\n",
    "\n",
    "$$x^{(i)}_j \\leftarrow \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "Here are some guidelines for the implementation:\n",
    "\n",
    "* The same transformation should be applied to train and test data.\n",
    "\n",
    "* The values $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation of\n",
    "the $j$th column (i.e., feature) in the **training data**. (Hint:\n",
    "there are numpy functions to compute these.)\n",
    "\n",
    "* Do not normalize the column of all ones. (Optional question: why?)\n",
    "\n",
    "* Use broadcasting to do the normalization--don't write for loops\n",
    "\n",
    "After normalizing both the training data and test data, follow the same steps as above to experiment with gradient descent using the *normalized* training and test data: print the value of the cost function, and create the same plots. Tune the step size and number of iterations again to make gradient descent converge as quickly as possible. How many iterations does it take to match the cost function value from ``normal_equations`` to two decimal places?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code for gradient descent with feature normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Write answer here: how many iterations?* **"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "094a986dbe4107eea991e75c3a97f40a2d3727af4659fa42ca665dd48f835325"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}